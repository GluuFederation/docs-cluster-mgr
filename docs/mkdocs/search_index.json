{
    "docs": [
        {
            "location": "/",
            "text": "Gluu Cluster Manager 3.0.2 Documentation\n#\n\n\nIntroduction\n#\n\n\nTBA",
            "title": "Home"
        },
        {
            "location": "/#gluu-cluster-manager-302-documentation",
            "text": "",
            "title": "Gluu Cluster Manager 3.0.2 Documentation"
        },
        {
            "location": "/#introduction",
            "text": "TBA",
            "title": "Introduction"
        },
        {
            "location": "/installation/installation/",
            "text": "Cluster Manager Installation Procedure\n#\n\n\nMinimum Requirements\n#\n\n\nServer:\n Ubuntu 16.04 (Xenial)\n\n\n\n\n\n\n\n\nCPU Unit\n\n\nRAM\n\n\nDisk Space\n\n\nProcessor Type\n\n\n\n\n\n\n\n\n\n\n2\n\n\n4GB\n\n\n40GB\n\n\n64 Bit\n\n\n\n\n\n\n\n\nPrepare a server with Ubuntu 16.04 (Xenial) already installed. Minimum recommendation resource is 4GB RAM, 2 CPU, and 40GB disk. Login via SSH to remote server for Cluster Manager installation.\n\n\nInstallation\n#\n\n\necho \"deb https://repo.gluu.org/ubuntu/ xenial-devel main\" > /etc/apt/sources.list.d/gluu-repo.list\ncurl https://repo.gluu.org/ubuntu/gluu-apt.key | apt-key add -\napt-get update\napt-get install -y gluu-cluster-mgr\n\n\n\n\nGenerate Public and Private Keys\n#\n\n\n\n\nNote\n\n\nSSH trust between Cluster Manager server (3) and the Gluu Servers (1) and (2)\n\n\n\n\nvia \nssh_keys\n is necessary for it to run operations remotely.\n\n\nCluster Manager runs as \ngluu\n user. Hence in order to run operation remotely WITHOUT a password prompt,\nthe public key (\n/home/gluu/.ssh/id_rsa.pub\n) of Cluster Manager\nshould be added to the \n<REMOTE_SERVER>.ssh/authorized_keys\n of all the servers it will communicate with.\n\n\nTo generate public and private key pair:\n\n\nsudo -u gluu mkdir /home/gluu/.ssh\nsudo -u gluu ssh-keygen -t rsa -b 4096 -C 'cluster-mgr'\n\n\n\n\nMake sure \nwe're NOT USING any passphrase\n when prompted.\n\n\nCopy the public key (\n/home/gluu/.ssh/id_rsa.pub\n) into local computer:\n\n\nscp root@<cluster-mgr-server>:/home/gluu/.ssh/id_rsa.pub </path/in/local/computer>\n\n\n\n\nIf using Windows machine and ssh using putty, you could use any scp app \nlike winscp to copy files to your local computer\nFrom local computer, copy the content of downloaded public key and append it to \nauthorized_keys\n \nof Gluu CE server:\n\n\ncat </the/above/id_rsa.pub> | ssh root@<gluu-server> 'cat >> .ssh/authorized_keys'\n\n\n\n\nMessage Consumer\n#\n\n\nLogin back to Cluster Manager server, then add new user and grant the \nprivileges to newly created user by login into MySQL console:\n\n\nmysql -u root -p\n\n\n\nType command below after successful MySQL console login:\n\n\nCREATE USER 'gluu'@'localhost' IDENTIFIED BY '<my-secret-password>';\nGRANT ALL PRIVILEGES ON gluu_log.* TO 'gluu'@'localhost';\n\n\n\nNote, change the \n<my-secret-password>\n to use our own password.\nAfterwards, exit from MySQL console login.\n\n\nIniatilize database for Message Consumer:\n\n\ncd /opt/message-consumer/conf\nmysql -u gluu -p < mysql_schema.sql\n\n\n\nModify lines below in \n/opt/message-consumer/conf/prod.properties\n file using text editor:\n\n\nspring.mysql.datasource.username=gluu\nspring.mysql.datasource.password=<my-secret-password>\n\n\n\nRestart \nmessage-consumer\n service to make sure Message Consumer loads updated configuration:\n\n\nsystemctl restart message-consumer\nsystemctl enable message-consumer\n\n\n\nCluster Manager\n#\n\n\nSync database schema (will create new db if not exist):\n\n\nAPP_MODE=prod clustermgr-cli db upgrade\n\n\n\nThe command above will create a database \n/opt/gluu-cluster-mgr/clustermgr.db\n.\nTo make sure Cluster Manager webapp has a sufficient access to database, run command below:\n\n\nchown gluu:gluu /opt/gluu-cluster-mgr/clustermgr.db\n\n\n\nGenerate random unique string:\n\n\ncat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1\n\n\n\n\n\nNote\n\n\nThe command above will print out string in terminal.\n\n\n\n\nCopy the value and keep it somewhere else for later use (i.e. backup plan).\n\n\nCreate \n/opt/gluu-cluster-mgr/instance/config.py\n and put line below:\n\n\nSECRET_KEY = \"<random-unique-string>\"\n\n\n\nNote, change the \n<random-unique-string>\n to use our own unique string.\n\n\nRestart \ngunicorn\n service:\n\n\nsystemctl restart gunicorn\nsystemctl enable gunicorn\n\n\n\nRestart \ncelery\n service to make sure background jobs runner connected to correct database:\n\n\nsystemctl restart celery\nsystemctl enable celery\n\n\n\nRestart \ncelerybeat\n service to make scheduled background jobs runner connected to correct database:\n\n\nsystemctl restart celerybeat\nsystemctl enable celerybeat\n\n\n\nNow logout from remote server.\n\n\nNote, Cluster Manager webapp is bind to \nlocalhost:6000\n in remote server.\nWe can use SSH tunneling to access it from web browser.\n\n\nssh -L 8080:localhost:6000 root@<cluster-mgr-server>\n\n\n\nAfterwards, type \nlocalhost:8080\n in web browser address bar.\n\n\noxEleven Application (optional)\n#\n\n\nAs an alternative of using JKS (the default backend) for storing oxAuth keys, we can also use \noxEleven\n.\n\n\nFirst things first, install docker as we need it for building oxEleven image:\n\n\ncurl -fsSL https://raw.githubusercontent.com/GluuFederation/cluster-tools/master/get_docker.sh | sh\n\n\n\nBuild oxEleven docker image:\n\n\ncd ~\ngit clone https://github.com/GluuFederation/gluu-docker.git\ncd gluu-docker\ngit checkout ce-3\ndocker build --rm=true --force-rm=true --tag=gluuox11 ubuntu/14.04/oxeleven\n\n\n\nGenerate random token (in this example, we will use UUID):\n\n\n$ cat /proc/sys/kernel/random/uuid\n\n\n\nKeep the token generated from process above for later use.\n\n\nRun oxEleven and bind it at \nhttp://<host>:8190\n:\n\n\ndocker run -d --name=ox11 --env OXUUID=random-token -p 8190:8080 --restart=always gluuox11\n\n\n\nNote, the access to oxEleven APIs will be protected by random token.",
            "title": "Install Cluster Manager"
        },
        {
            "location": "/installation/installation/#cluster-manager-installation-procedure",
            "text": "",
            "title": "Cluster Manager Installation Procedure"
        },
        {
            "location": "/installation/installation/#minimum-requirements",
            "text": "Server:  Ubuntu 16.04 (Xenial)     CPU Unit  RAM  Disk Space  Processor Type      2  4GB  40GB  64 Bit     Prepare a server with Ubuntu 16.04 (Xenial) already installed. Minimum recommendation resource is 4GB RAM, 2 CPU, and 40GB disk. Login via SSH to remote server for Cluster Manager installation.",
            "title": "Minimum Requirements"
        },
        {
            "location": "/installation/installation/#installation",
            "text": "echo \"deb https://repo.gluu.org/ubuntu/ xenial-devel main\" > /etc/apt/sources.list.d/gluu-repo.list\ncurl https://repo.gluu.org/ubuntu/gluu-apt.key | apt-key add -\napt-get update\napt-get install -y gluu-cluster-mgr",
            "title": "Installation"
        },
        {
            "location": "/installation/installation/#generate-public-and-private-keys",
            "text": "Note  SSH trust between Cluster Manager server (3) and the Gluu Servers (1) and (2)   via  ssh_keys  is necessary for it to run operations remotely.  Cluster Manager runs as  gluu  user. Hence in order to run operation remotely WITHOUT a password prompt,\nthe public key ( /home/gluu/.ssh/id_rsa.pub ) of Cluster Manager\nshould be added to the  <REMOTE_SERVER>.ssh/authorized_keys  of all the servers it will communicate with.  To generate public and private key pair:  sudo -u gluu mkdir /home/gluu/.ssh\nsudo -u gluu ssh-keygen -t rsa -b 4096 -C 'cluster-mgr'  Make sure  we're NOT USING any passphrase  when prompted.  Copy the public key ( /home/gluu/.ssh/id_rsa.pub ) into local computer:  scp root@<cluster-mgr-server>:/home/gluu/.ssh/id_rsa.pub </path/in/local/computer>  If using Windows machine and ssh using putty, you could use any scp app \nlike winscp to copy files to your local computer\nFrom local computer, copy the content of downloaded public key and append it to  authorized_keys  \nof Gluu CE server:  cat </the/above/id_rsa.pub> | ssh root@<gluu-server> 'cat >> .ssh/authorized_keys'",
            "title": "Generate Public and Private Keys"
        },
        {
            "location": "/installation/installation/#message-consumer",
            "text": "Login back to Cluster Manager server, then add new user and grant the \nprivileges to newly created user by login into MySQL console:  mysql -u root -p  Type command below after successful MySQL console login:  CREATE USER 'gluu'@'localhost' IDENTIFIED BY '<my-secret-password>';\nGRANT ALL PRIVILEGES ON gluu_log.* TO 'gluu'@'localhost';  Note, change the  <my-secret-password>  to use our own password.\nAfterwards, exit from MySQL console login.  Iniatilize database for Message Consumer:  cd /opt/message-consumer/conf\nmysql -u gluu -p < mysql_schema.sql  Modify lines below in  /opt/message-consumer/conf/prod.properties  file using text editor:  spring.mysql.datasource.username=gluu\nspring.mysql.datasource.password=<my-secret-password>  Restart  message-consumer  service to make sure Message Consumer loads updated configuration:  systemctl restart message-consumer\nsystemctl enable message-consumer",
            "title": "Message Consumer"
        },
        {
            "location": "/installation/installation/#cluster-manager",
            "text": "Sync database schema (will create new db if not exist):  APP_MODE=prod clustermgr-cli db upgrade  The command above will create a database  /opt/gluu-cluster-mgr/clustermgr.db .\nTo make sure Cluster Manager webapp has a sufficient access to database, run command below:  chown gluu:gluu /opt/gluu-cluster-mgr/clustermgr.db  Generate random unique string:  cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1   Note  The command above will print out string in terminal.   Copy the value and keep it somewhere else for later use (i.e. backup plan).  Create  /opt/gluu-cluster-mgr/instance/config.py  and put line below:  SECRET_KEY = \"<random-unique-string>\"  Note, change the  <random-unique-string>  to use our own unique string.  Restart  gunicorn  service:  systemctl restart gunicorn\nsystemctl enable gunicorn  Restart  celery  service to make sure background jobs runner connected to correct database:  systemctl restart celery\nsystemctl enable celery  Restart  celerybeat  service to make scheduled background jobs runner connected to correct database:  systemctl restart celerybeat\nsystemctl enable celerybeat  Now logout from remote server.  Note, Cluster Manager webapp is bind to  localhost:6000  in remote server.\nWe can use SSH tunneling to access it from web browser.  ssh -L 8080:localhost:6000 root@<cluster-mgr-server>  Afterwards, type  localhost:8080  in web browser address bar.",
            "title": "Cluster Manager"
        },
        {
            "location": "/installation/installation/#oxeleven-application-optional",
            "text": "As an alternative of using JKS (the default backend) for storing oxAuth keys, we can also use  oxEleven .  First things first, install docker as we need it for building oxEleven image:  curl -fsSL https://raw.githubusercontent.com/GluuFederation/cluster-tools/master/get_docker.sh | sh  Build oxEleven docker image:  cd ~\ngit clone https://github.com/GluuFederation/gluu-docker.git\ncd gluu-docker\ngit checkout ce-3\ndocker build --rm=true --force-rm=true --tag=gluuox11 ubuntu/14.04/oxeleven  Generate random token (in this example, we will use UUID):  $ cat /proc/sys/kernel/random/uuid  Keep the token generated from process above for later use.  Run oxEleven and bind it at  http://<host>:8190 :  docker run -d --name=ox11 --env OXUUID=random-token -p 8190:8080 --restart=always gluuox11  Note, the access to oxEleven APIs will be protected by random token.",
            "title": "oxEleven Application (optional)"
        },
        {
            "location": "/installation/upgrade/",
            "text": "Cluster Manager Upgrade Guide\n#\n\n\nUpgrade using apt-get\n#\n\n\nTo upgrade Gluu Cluster Manager using apt-get commands\n\n\n` #apt-get clean`\n\n` #apt-get update`\n\n` #apt-get install python-cluster-mgr`\n\n\n\nTo check for the latest policy use below command\n\n\n` #apt-cache policy python-cluster-mgr`\n\n\n\nRestart the services for Cluster Manager using below commands.\n\n\n` #systemctl restart gunicorn`\n\n` #systemctl restart celery`\n\n` #systemctl restart celerybeat`\n\n\n\nNow your existing package has been upgraded with new build.\n\n\nManual Upgrade\n#\n\n\nTo upgrade Gluu Cluster Manager manually, use below commands:\n\n\nNavigate to the Cluster Manager install directory.\n\n\n` #cd /usr/lib/python2.7/dist-packages/clustermgr`\n\n\n\nGet the latest package and install\n\n\n` #wget https://github.com/GluuFederation/cluster-mgr/raw/be2679d9fc7d53076df386847374f3ef68087c50/clustermgr/tasks.py -O tasks.py`\n\n\n\n#rm tasks.pyc\n\n\nRestart Cluster Manager Services\n\n\n` #systemctl restart gunicorn`\n\n` #systemctl restart celery`\n\n` #systemctl restart celerybeat`",
            "title": "Upgrade Gluu Cluster Manager"
        },
        {
            "location": "/installation/upgrade/#cluster-manager-upgrade-guide",
            "text": "",
            "title": "Cluster Manager Upgrade Guide"
        },
        {
            "location": "/installation/upgrade/#upgrade-using-apt-get",
            "text": "To upgrade Gluu Cluster Manager using apt-get commands  ` #apt-get clean`\n\n` #apt-get update`\n\n` #apt-get install python-cluster-mgr`  To check for the latest policy use below command  ` #apt-cache policy python-cluster-mgr`  Restart the services for Cluster Manager using below commands.  ` #systemctl restart gunicorn`\n\n` #systemctl restart celery`\n\n` #systemctl restart celerybeat`  Now your existing package has been upgraded with new build.",
            "title": "Upgrade using apt-get"
        },
        {
            "location": "/installation/upgrade/#manual-upgrade",
            "text": "To upgrade Gluu Cluster Manager manually, use below commands:  Navigate to the Cluster Manager install directory.  ` #cd /usr/lib/python2.7/dist-packages/clustermgr`  Get the latest package and install  ` #wget https://github.com/GluuFederation/cluster-mgr/raw/be2679d9fc7d53076df386847374f3ef68087c50/clustermgr/tasks.py -O tasks.py`  #rm tasks.pyc  Restart Cluster Manager Services  ` #systemctl restart gunicorn`\n\n` #systemctl restart celery`\n\n` #systemctl restart celerybeat`",
            "title": "Manual Upgrade"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/",
            "text": "Preparing Gluu CE for Replication\n#\n\n\nPreface\n#\n\n\nThis page is about OpenLDAP replication used as the data storage for Gluu Server. \nOpenLDAP supports many different types of replication \n\ntopological setups\n. \nThe Cluster Manager application by Gluu supports two of them.\n\n\n\n\nDelta-Synrepl\n - for high availability single master (read/write) multiple consumer (read only) setup.\n\n\nMirror Mode\n - for fall back double master (read/write) multiple consumers under each master (read only) setup.\nThis document outline the setup procedure for both mode of operations.\n\n\n\n\nMigration Procedure for Mirror Mode\n#\n\n\n\n\nPre-requisites\n#\n\n\n\n\nExisting Gluu Server 3.0.1 installation. Refer \ninstallation guide\n for details.\n\n\nA new server to act as mirror.\n\n\nCluster Manager installed and accessible from web browser.\n\n\n\n\nServer 1 (Existing Gluu Server)\n#\n\n\n\n\n\n\nBackup Data:\n Login login to the chroot environment of the Gluu server. \n    While inside the chroot, stop LDAP server and export its data for backup\n\n\nservice solserver stop\n/opt/symas/bin/slapcat -l alldata.ldif\n\n\n\n\n\n\nEdit the file\n \n/opt/symas/etc/openldap/symas-openldap.conf\n \n    to allow servers within chroot to connect to LDAP and make OpenLDAP to use OLC (On-Line Configuration).\n    As Gluu recommends to use FQDN or IP for its connections to LDAP.\n\n\nChange the values of \nHOST_LIST\n and \nEXTRA_SLAPD_ARGS\n\n\nHOST_LIST=\"ldaps://127.0.0.1:1636/\"\nto \nHOST_LIST=\"ldaps://127.0.0.1:1636/ ldaps://<server_ip>:1636\"\n\n\nEXTRA_SLAPD_ARGS=\" \"\nto\nEXTRA_SLAPD_ARGS=\"-F /opt/symas/etc/openldap/slapd.d\"\n\n\n\n\n\n\nGenerate new SSL certificate for OpenLDAP:\n The default certificate for OpenLDAP in Gluu Server is for localhost only, we need to generate a hostname based certificate for wider access. (Skip/Modify this step if you are using a different CA system). \nImportant:\n Common Name (e.g. server FQDN or YOUR name) should be your hostname.\n\n\n\n\nWarning\nEnsure the Common Name here is your full hostname and NOT localhost.\n\n\n\n\n\n\n# cd /etc/certs\n\n\n# mkdir old_certs\n\n\n# mv openldap.* old_certs/\n\n\n# /usr/bin/openssl genrsa -des3 -out /etc/certs/openldap.key.orig 2048\n\n\n# /usr/bin/openssl rsa -in /etc/certs/openldap.key.orig -out /etc/certs/openldap.key\n\n\n# /usr/bin/openssl req -new -key /etc/certs/openldap.key -out /etc/certs/openldap.csr\n\n\n# /usr/bin/openssl x509 -req -days 365 -in /etc/certs/openldap.csr -signkey /etc/certs/openldap.key -out /etc/certs/openldap.crt\n\n\nChange the \n of the \n in the command below\n\n\n#/opt/jre/bin/keytool -import -trustcacerts -alias <server1>_openldap_2 -file /etc/certs/openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt\n\n\ncp openldap.crt openldap.pem\n\n\n\n\n\n\nConfigure oxAuth to both the servers:\n \n    Add the hostname (FQDN) of both the servers to \n/etc/gluu/conf/ox-ldap.properties\n as shown below. \n    This points the oxAuth to the two LDAP servers it can connect to, providing fallback in case \n    of one server failing.\n\n\nAdd the server FQDN to the value \nservers\n\n\nservers: <hostname>:1636,<server_2_hostname>:1636\n\n\n\n\n\n\nCreate a backup of some files necessary for replication:\n\n\n# cd ~\n\n\n# mkdir repfiles\n\n\n# cd repfiles\n\n\n# cp /etc/certs/openldap.crt .\n\n\n# cp -r /etc/gluu/conf/ .\n\n\n# cd ..\n\n\n# tar -czf repfiles.tar.gz repfiles\n\n\n\n\n\n\nCopy the backup files to local computer, we will use this to configure the second server\n\n\nscp root@server1:/opt/gluu-server-3.0.1/root/repfiles.tar.gz </location/in/local/server>\n\n\n\n\n\n\n\n\nNote\n\n\nThere is no need to start the LDAP server (service solserver) now, it would be configured and started by the Cluster Manager later.\n\n\n\n\nServer 2 (Mirror Server)\n#\n\n\n\n\n\n\nInstall CE package on server 2\n. \n    The aim here is to setup the LDAP and oxAuth only so, when running \nsetup.py\n mark only \"Install oxAuth\", \"LDAP\" and \"JCE\"  as True and everything as false.\n\n\n\n\n\n\nCopy the backfiles from server 1 to server 2:\n\n\nscp repfiles.tar.gz root@server2:/opt/gluu-server-3.0.1/root/\n\n\n\n\n\n\nReplace the existing files with backup ones:\n This sets up the second gluu-server to act like the twin of the first one\n\n\n#ssh root@server_2\n\n\n#service gluu-server-3.0.1 login\n\n\n#tar -xvf repfiles.tar.gz\n\n\n#cd repfiles\n\n\n#rm -rf /etc/gluu/conf/\n\n\n#cp -r conf /etc/gluu/\n\n\n#/opt/jre/bin/keytool -import -trustcacerts -alias <server_1_hostname>_openldap_2 -file openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt\n\n\n\n\n\n\nReplace the OpenLDAP certificates:\n\n\n#cd /etc/certs\n\n\n#mkdir old_certs\n\n\n#mv openldap.* old_certs/\n\n\n#/usr/bin/openssl genrsa -des3 -out /etc/certs/openldap.key.orig 2048\n\n\n#/usr/bin/openssl rsa -in /etc/certs/openldap.key.orig -out /etc/certs/openldap.key\n\n\nEnsure the Common Name here is your full \nhostname\n and NOT localhost\n\n\n#/usr/bin/openssl req -new -key /etc/certs/openldap.key -out /etc/certs/openldap.csr\n\n\n#/usr/bin/openssl x509 -req -days 365 -in /etc/certs/openldap.csr -signkey /etc/certs/openldap.key -out /etc/certs/openldap.crt\n\n\nChange the \nhostname\n in the command below\n\n\n#/opt/jre/bin/keytool -import -trustcacerts -alias <hostname>_openldap_2 -file /etc/certs/openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt\n\n\n#cp openldap.crt openldap.pem\n\n\n\n\n\n\nEdit the file\n \n/opt/symas/etc/openldap/symas-openldap.conf\n \n    to allow servers within chroot to connect to LDAP and make OpenLDAP to use OLC (On-Line Configuration).\n    As Gluu recommends to use FQDN or IP for its connections to LDAP.\n\n\nChange the values of \nHOST_LIST\n and \nEXTRA_SLAPD_ARGS\n\n\nHOST_LIST=\"ldaps://127.0.0.1:1636/\"\nto \nHOST_LIST=\"ldaps://127.0.0.1:1636/ ldaps://<server_ip>:1636\"\n\n\nEXTRA_SLAPD_ARGS=\" \"\nto\nEXTRA_SLAPD_ARGS=\"-F /opt/symas/etc/openldap/slapd.d\"\n\n\n\n\n\n\nClean up the existing LDAP data files:\n The data would be replicated from server 1 when configured.\n\n\nrm -rf /opt/gluu/data/\n\n\n\n\n\n\nMirror Server 1 to Server 2\n#\n\n\nCopy \nopenldap.crt\n from server 2 to server 1 and import it into the truststore for the apps like oxAuth to connect to the LDAP in server 2.\n\n\nscp root@server2:/opt/gluu-server-3.0.1/etc/certs/openldap.crt .\nscp openldap.crt root@server1:/opt/gluu-server-3.0.1/root/server2_openldap.crt\nssh root@server1\nservice gluu-server-3.0.1 login\n/opt/jre/bin/keytool -import -trustcacerts -alias <server2>_openldap_2 -file server2_openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt\n\n\n\n\nNow both the servers are ready to be configured to a cluster. \nAfter you get the Cluster Manager installed, follow the \nMirror Mode\n in \nreplication setup guide",
            "title": "Configuring Gluu CE for Replication"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#preparing-gluu-ce-for-replication",
            "text": "",
            "title": "Preparing Gluu CE for Replication"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#preface",
            "text": "This page is about OpenLDAP replication used as the data storage for Gluu Server. \nOpenLDAP supports many different types of replication  topological setups . \nThe Cluster Manager application by Gluu supports two of them.   Delta-Synrepl  - for high availability single master (read/write) multiple consumer (read only) setup.  Mirror Mode  - for fall back double master (read/write) multiple consumers under each master (read only) setup.\nThis document outline the setup procedure for both mode of operations.",
            "title": "Preface"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#migration-procedure-for-mirror-mode",
            "text": "",
            "title": "Migration Procedure for Mirror Mode"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#pre-requisites",
            "text": "Existing Gluu Server 3.0.1 installation. Refer  installation guide  for details.  A new server to act as mirror.  Cluster Manager installed and accessible from web browser.",
            "title": "Pre-requisites"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#server-1-existing-gluu-server",
            "text": "Backup Data:  Login login to the chroot environment of the Gluu server. \n    While inside the chroot, stop LDAP server and export its data for backup  service solserver stop\n/opt/symas/bin/slapcat -l alldata.ldif    Edit the file   /opt/symas/etc/openldap/symas-openldap.conf  \n    to allow servers within chroot to connect to LDAP and make OpenLDAP to use OLC (On-Line Configuration).\n    As Gluu recommends to use FQDN or IP for its connections to LDAP.  Change the values of  HOST_LIST  and  EXTRA_SLAPD_ARGS  HOST_LIST=\"ldaps://127.0.0.1:1636/\"\nto \nHOST_LIST=\"ldaps://127.0.0.1:1636/ ldaps://<server_ip>:1636\"  EXTRA_SLAPD_ARGS=\" \"\nto\nEXTRA_SLAPD_ARGS=\"-F /opt/symas/etc/openldap/slapd.d\"    Generate new SSL certificate for OpenLDAP:  The default certificate for OpenLDAP in Gluu Server is for localhost only, we need to generate a hostname based certificate for wider access. (Skip/Modify this step if you are using a different CA system).  Important:  Common Name (e.g. server FQDN or YOUR name) should be your hostname.   Warning Ensure the Common Name here is your full hostname and NOT localhost.    # cd /etc/certs  # mkdir old_certs  # mv openldap.* old_certs/  # /usr/bin/openssl genrsa -des3 -out /etc/certs/openldap.key.orig 2048  # /usr/bin/openssl rsa -in /etc/certs/openldap.key.orig -out /etc/certs/openldap.key  # /usr/bin/openssl req -new -key /etc/certs/openldap.key -out /etc/certs/openldap.csr  # /usr/bin/openssl x509 -req -days 365 -in /etc/certs/openldap.csr -signkey /etc/certs/openldap.key -out /etc/certs/openldap.crt  Change the   of the   in the command below  #/opt/jre/bin/keytool -import -trustcacerts -alias <server1>_openldap_2 -file /etc/certs/openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt  cp openldap.crt openldap.pem    Configure oxAuth to both the servers:  \n    Add the hostname (FQDN) of both the servers to  /etc/gluu/conf/ox-ldap.properties  as shown below. \n    This points the oxAuth to the two LDAP servers it can connect to, providing fallback in case \n    of one server failing.  Add the server FQDN to the value  servers  servers: <hostname>:1636,<server_2_hostname>:1636    Create a backup of some files necessary for replication:  # cd ~  # mkdir repfiles  # cd repfiles  # cp /etc/certs/openldap.crt .  # cp -r /etc/gluu/conf/ .  # cd ..  # tar -czf repfiles.tar.gz repfiles    Copy the backup files to local computer, we will use this to configure the second server  scp root@server1:/opt/gluu-server-3.0.1/root/repfiles.tar.gz </location/in/local/server>     Note  There is no need to start the LDAP server (service solserver) now, it would be configured and started by the Cluster Manager later.",
            "title": "Server 1 (Existing Gluu Server)"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#server-2-mirror-server",
            "text": "Install CE package on server 2 . \n    The aim here is to setup the LDAP and oxAuth only so, when running  setup.py  mark only \"Install oxAuth\", \"LDAP\" and \"JCE\"  as True and everything as false.    Copy the backfiles from server 1 to server 2:  scp repfiles.tar.gz root@server2:/opt/gluu-server-3.0.1/root/    Replace the existing files with backup ones:  This sets up the second gluu-server to act like the twin of the first one  #ssh root@server_2  #service gluu-server-3.0.1 login  #tar -xvf repfiles.tar.gz  #cd repfiles  #rm -rf /etc/gluu/conf/  #cp -r conf /etc/gluu/  #/opt/jre/bin/keytool -import -trustcacerts -alias <server_1_hostname>_openldap_2 -file openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt    Replace the OpenLDAP certificates:  #cd /etc/certs  #mkdir old_certs  #mv openldap.* old_certs/  #/usr/bin/openssl genrsa -des3 -out /etc/certs/openldap.key.orig 2048  #/usr/bin/openssl rsa -in /etc/certs/openldap.key.orig -out /etc/certs/openldap.key  Ensure the Common Name here is your full  hostname  and NOT localhost  #/usr/bin/openssl req -new -key /etc/certs/openldap.key -out /etc/certs/openldap.csr  #/usr/bin/openssl x509 -req -days 365 -in /etc/certs/openldap.csr -signkey /etc/certs/openldap.key -out /etc/certs/openldap.crt  Change the  hostname  in the command below  #/opt/jre/bin/keytool -import -trustcacerts -alias <hostname>_openldap_2 -file /etc/certs/openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt  #cp openldap.crt openldap.pem    Edit the file   /opt/symas/etc/openldap/symas-openldap.conf  \n    to allow servers within chroot to connect to LDAP and make OpenLDAP to use OLC (On-Line Configuration).\n    As Gluu recommends to use FQDN or IP for its connections to LDAP.  Change the values of  HOST_LIST  and  EXTRA_SLAPD_ARGS  HOST_LIST=\"ldaps://127.0.0.1:1636/\"\nto \nHOST_LIST=\"ldaps://127.0.0.1:1636/ ldaps://<server_ip>:1636\"  EXTRA_SLAPD_ARGS=\" \"\nto\nEXTRA_SLAPD_ARGS=\"-F /opt/symas/etc/openldap/slapd.d\"    Clean up the existing LDAP data files:  The data would be replicated from server 1 when configured.  rm -rf /opt/gluu/data/",
            "title": "Server 2 (Mirror Server)"
        },
        {
            "location": "/configuration/configuring-GluuCE-Cluster/#mirror-server-1-to-server-2",
            "text": "Copy  openldap.crt  from server 2 to server 1 and import it into the truststore for the apps like oxAuth to connect to the LDAP in server 2.  scp root@server2:/opt/gluu-server-3.0.1/etc/certs/openldap.crt .\nscp openldap.crt root@server1:/opt/gluu-server-3.0.1/root/server2_openldap.crt\nssh root@server1\nservice gluu-server-3.0.1 login\n/opt/jre/bin/keytool -import -trustcacerts -alias <server2>_openldap_2 -file server2_openldap.crt -keystore /opt/jre/jre/lib/security/cacerts -storepass changeit -noprompt  Now both the servers are ready to be configured to a cluster. \nAfter you get the Cluster Manager installed, follow the  Mirror Mode  in  replication setup guide",
            "title": "Mirror Server 1 to Server 2"
        },
        {
            "location": "/configuration/configuring-oxAuth-Logs/",
            "text": "Configuring Message Consumer\n#\n\n\nIn clustered oxAuth setup, all server logs can be stored into ActiveMQ via \nMessage Consumer\n application.\nTo see those logs, we need to connect Cluster Manager to Message Consumer application.\nMake sure that oxAuth are configured to \nsend the log to ActiveMQ\n.\n\n\nClick \noxAuth Logging\n link in left sidebar menu, a new form will be displayed as shown below:\n\n\n[[/img/oxauth-log/oxauth-log-config.png|Message Consumer empty URL]]\n\n\nNote that if the URL is empty or unreachable, a warning message will be displayed at the top of the page.\n\n\nMessage Consumer URL\n#\n\n\nThe URL specifies full URL of Message Consumer application (REST API).\nMessage Consumer is installed by default when we install \ngluu-cluster-mgr\n package, and it is running at \nhttp://localhost:9339\n.\n\n\n[[/img/oxauth-log/oxauth-log-msgcon-url.png|Message Consumer URL]]\n\n\nSave the URL by clicking \nSave Config\n button.\nIf URL is reachable, then 2 new links will be displayed, \nAudit Logs\n and \nServer Logs\n.\nIn this example, we will focus on server logs. Click the \nServer Logs\n link.\n\n\noxAuth Server Logs\n#\n\n\nIf oxAuth logs are available in ActiveMQ, Cluster Manager will show them in paginated list.\n\n\n[[/img/oxauth-log/oxauth-log-server-log-list.png|oxAuth server logs list]]\n\n\nEach log details can be viewed by clicking \nView\n under the Details table header.\nAn example of server log details is shown below:\n\n\n[[/img/oxauth-log/oxauth-log-server-log-item.png|oxAuth server log details]]",
            "title": "Configuring oxAuth logs"
        },
        {
            "location": "/configuration/configuring-oxAuth-Logs/#configuring-message-consumer",
            "text": "In clustered oxAuth setup, all server logs can be stored into ActiveMQ via  Message Consumer  application.\nTo see those logs, we need to connect Cluster Manager to Message Consumer application.\nMake sure that oxAuth are configured to  send the log to ActiveMQ .  Click  oxAuth Logging  link in left sidebar menu, a new form will be displayed as shown below:  [[/img/oxauth-log/oxauth-log-config.png|Message Consumer empty URL]]  Note that if the URL is empty or unreachable, a warning message will be displayed at the top of the page.",
            "title": "Configuring Message Consumer"
        },
        {
            "location": "/configuration/configuring-oxAuth-Logs/#message-consumer-url",
            "text": "The URL specifies full URL of Message Consumer application (REST API).\nMessage Consumer is installed by default when we install  gluu-cluster-mgr  package, and it is running at  http://localhost:9339 .  [[/img/oxauth-log/oxauth-log-msgcon-url.png|Message Consumer URL]]  Save the URL by clicking  Save Config  button.\nIf URL is reachable, then 2 new links will be displayed,  Audit Logs  and  Server Logs .\nIn this example, we will focus on server logs. Click the  Server Logs  link.",
            "title": "Message Consumer URL"
        },
        {
            "location": "/configuration/configuring-oxAuth-Logs/#oxauth-server-logs",
            "text": "If oxAuth logs are available in ActiveMQ, Cluster Manager will show them in paginated list.  [[/img/oxauth-log/oxauth-log-server-log-list.png|oxAuth server logs list]]  Each log details can be viewed by clicking  View  under the Details table header.\nAn example of server log details is shown below:  [[/img/oxauth-log/oxauth-log-server-log-item.png|oxAuth server log details]]",
            "title": "oxAuth Server Logs"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/",
            "text": "Configuring oxAuth Key Rotation\n#\n\n\nKey rotation consists of several processes:\n\n\n\n\nGenerating private keys (stored in \noxEleven\n or \nJKS\n backend)\n\n\nGenerating public keys and save them to LDAP.\n\n\nRunning scheduled background job that executes process 1 and 2.\n\n\n\n\nTo configure oxAuth key rotation, click \noxAuth Key Rotation\n in left sidebar menu.\n\n\nKey Rotation Interval\n#\n\n\nThis value determines the interval (in days) of automatic key rotation after configuration has been saved.\nFor example, if key rotation config are saved on 21st August, the next key rotation will occur automatically\nat 23rd August and so on.\n\n\n[[/img/key-rotation/oxauth-key-interval-marker.png|Key Rotation Interval]]\n\n\nKey Rotation Type\n#\n\n\nThere are 2 backend types we can choose, \noxEleven\n and \nJKS\n (Java keystore).\n\n\nJKS Backend\n#\n\n\n[[/img/key-rotation/oxauth-key-jks-radio-marker.png|Key Rotation JKS Type]]\n\n\nWhen we choose JKS backend, a new section will appear as shown below.\nNote, we need to add oxAuth server(s) where JKS file will be distributed to all available servers.\nTo add oxAuth server, type the hostname or IP address of the server in the text field.\nAfterwards, click \nAdd oxAuth server\n button.\n\n\n[[/img/key-rotation/oxauth-add-oxauth-marker.png|Add oxAuth server]]\n\n\nIf we want to remove oxAuth server, click \nRemove?\n checkbox for desired server and then click \nRemove selected\n button.\n\n\n[[/img/key-rotation/oxauth-remove-oxauth-marker.png|Remove oxAuth server]]\n\n\noxEleven Backend\n#\n\n\n[[/img/key-rotation/oxauth-key-ox11-radio.png|Key Rotation oxEleven Type]]\n\n\nWhen we choose oxEleven backend, a new section will appear as shown below:\n\n\n[[/img/key-rotation/oxauth-key-ox11-config.png|oxEleven config]]\n\n\nNote, refer to \noxEleven setup\n for details.\n\n\nEnter the URL of oxEleven in \nhttp://host:port\n format.\nWe already know that oxEleven is installed in the same host,\nhence we can use IP address from \neth0\n interface and port 8190.\n\n\nEnter the random token mentioned in oxEleven setup section in link above.\n\n\nLDAP Integration\n#\n\n\nAs public keys are stored in LDAP, we need to specify \ninum appliance\n for searching and replacing desired entries in LDAP.\nWe can find the \ninum appliance\n in \n/opt/gluu-server-3.0.1/install/community-edition-setup/setup.properties.last\n inside the Gluu Server (1).\nLocate the line \ninumAppliance\n inside that file as shown below:\n\n\n# /opt/gluu-server-3.0.1/install/community-edition-setup/setup.properties.last file\ninumApplianceFN=56EF0E9AF67AB15B0002AC3A8E0E\ninumAppliance=@!56EF.0E9A.F67A.B15B!0002!AC3A.8E0E # this is the inum appliance\n\n\n\nCopy the value and paste into the form field:\n\n\n[[/img/key-rotation/oxauth-inum-appliance.png|Configure Inum Appliance]]\n\n\nOnce we have entered correct Inum Appliance, click \nRotate Key\n at the bottom of the form.\n\n\nMonitoring oxAuth Key Rotation\n#\n\n\nAs key rotation process is running as background job, to monitor the result (or error), we can tail a log file.\n\n\ntailf /var/log/celery/w1-1.log\n\n\n\nHere's an example of successful key rotation logged in \n/var/log/celery/w1-1.log\n when using \njks\n backend type:\n\n\n[2017-05-04 12:24:40,073: WARNING/Worker-1] [root@128.199.116.221] Executing task '_copy_jks'\n[2017-05-04 12:24:40,091: INFO/Worker-1] Connected (version 2.0, client OpenSSH_7.2p2)\n[2017-05-04 12:24:40,312: INFO/Worker-1] Authentication (publickey) successful!\n[2017-05-04 12:24:40,554: INFO/Worker-1] [chan 0] Opened sftp connection (server version 3)\n[2017-05-04 12:24:40,559: WARNING/Worker-1] [root@128.199.116.221] put: /opt/gluu-cluster-mgr/oxauth-keys.jks -> /opt/gluu-server-3.0.1/etc/certs/oxauth-keys.jks\n[2017-05-04 12:24:40,563: INFO/Worker-1] [chan 0] sftp session closed.\n[2017-05-04 12:24:40,564: WARNING/Worker-1] JKS file has been copied to 128.199.116.221\n[2017-05-04 12:26:31,410: WARNING/Worker-1] key rotation task will be executed approximately at 2017-05-05 12:24:40.052739 UTC\n\n\n\nHere's an example of successful key rotation logged in \n/var/log/celery/w1-1.log\n when using \noxeleven\n backend type:\n\n\n[2017-05-04 12:47:32,727: WARNING/Worker-4] key rotation task will be executed approximately at 2017-05-05 12:24:40.052739 UTC\n[2017-05-04 12:49:07,780: WARNING/Worker-4] deleting old keys\n[2017-05-04 12:49:07,798: INFO/Worker-4] Starting new HTTP connection (1): 128.199.225.90\n[2017-05-04 12:49:08,068: WARNING/Worker-4] obtaining new keys\n[2017-05-04 12:49:08,070: INFO/Worker-4] Starting new HTTP connection (1): 128.199.225.90\n[2017-05-04 12:49:08,707: WARNING/Worker-4] pub keys has been updated\n\n\n\nNote, if we don't see logs about key rotation in \n/var/log/celery/w1-1.log\n, we can try to tail the other log files; \n/var/log/celery/w1-2.log\n, \n/var/log/celery/w1-3.log\n, or \n/var/log/celery/w1-4.log\n.",
            "title": "Configuring oxAuth Key Rotation"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#configuring-oxauth-key-rotation",
            "text": "Key rotation consists of several processes:   Generating private keys (stored in  oxEleven  or  JKS  backend)  Generating public keys and save them to LDAP.  Running scheduled background job that executes process 1 and 2.   To configure oxAuth key rotation, click  oxAuth Key Rotation  in left sidebar menu.",
            "title": "Configuring oxAuth Key Rotation"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#key-rotation-interval",
            "text": "This value determines the interval (in days) of automatic key rotation after configuration has been saved.\nFor example, if key rotation config are saved on 21st August, the next key rotation will occur automatically\nat 23rd August and so on.  [[/img/key-rotation/oxauth-key-interval-marker.png|Key Rotation Interval]]",
            "title": "Key Rotation Interval"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#key-rotation-type",
            "text": "There are 2 backend types we can choose,  oxEleven  and  JKS  (Java keystore).",
            "title": "Key Rotation Type"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#jks-backend",
            "text": "[[/img/key-rotation/oxauth-key-jks-radio-marker.png|Key Rotation JKS Type]]  When we choose JKS backend, a new section will appear as shown below.\nNote, we need to add oxAuth server(s) where JKS file will be distributed to all available servers.\nTo add oxAuth server, type the hostname or IP address of the server in the text field.\nAfterwards, click  Add oxAuth server  button.  [[/img/key-rotation/oxauth-add-oxauth-marker.png|Add oxAuth server]]  If we want to remove oxAuth server, click  Remove?  checkbox for desired server and then click  Remove selected  button.  [[/img/key-rotation/oxauth-remove-oxauth-marker.png|Remove oxAuth server]]",
            "title": "JKS Backend"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#oxeleven-backend",
            "text": "[[/img/key-rotation/oxauth-key-ox11-radio.png|Key Rotation oxEleven Type]]  When we choose oxEleven backend, a new section will appear as shown below:  [[/img/key-rotation/oxauth-key-ox11-config.png|oxEleven config]]  Note, refer to  oxEleven setup  for details.  Enter the URL of oxEleven in  http://host:port  format.\nWe already know that oxEleven is installed in the same host,\nhence we can use IP address from  eth0  interface and port 8190.  Enter the random token mentioned in oxEleven setup section in link above.",
            "title": "oxEleven Backend"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#ldap-integration",
            "text": "As public keys are stored in LDAP, we need to specify  inum appliance  for searching and replacing desired entries in LDAP.\nWe can find the  inum appliance  in  /opt/gluu-server-3.0.1/install/community-edition-setup/setup.properties.last  inside the Gluu Server (1).\nLocate the line  inumAppliance  inside that file as shown below:  # /opt/gluu-server-3.0.1/install/community-edition-setup/setup.properties.last file\ninumApplianceFN=56EF0E9AF67AB15B0002AC3A8E0E\ninumAppliance=@!56EF.0E9A.F67A.B15B!0002!AC3A.8E0E # this is the inum appliance  Copy the value and paste into the form field:  [[/img/key-rotation/oxauth-inum-appliance.png|Configure Inum Appliance]]  Once we have entered correct Inum Appliance, click  Rotate Key  at the bottom of the form.",
            "title": "LDAP Integration"
        },
        {
            "location": "/configuration/configuring-oxAuth-Key-Rotation/#monitoring-oxauth-key-rotation",
            "text": "As key rotation process is running as background job, to monitor the result (or error), we can tail a log file.  tailf /var/log/celery/w1-1.log  Here's an example of successful key rotation logged in  /var/log/celery/w1-1.log  when using  jks  backend type:  [2017-05-04 12:24:40,073: WARNING/Worker-1] [root@128.199.116.221] Executing task '_copy_jks'\n[2017-05-04 12:24:40,091: INFO/Worker-1] Connected (version 2.0, client OpenSSH_7.2p2)\n[2017-05-04 12:24:40,312: INFO/Worker-1] Authentication (publickey) successful!\n[2017-05-04 12:24:40,554: INFO/Worker-1] [chan 0] Opened sftp connection (server version 3)\n[2017-05-04 12:24:40,559: WARNING/Worker-1] [root@128.199.116.221] put: /opt/gluu-cluster-mgr/oxauth-keys.jks -> /opt/gluu-server-3.0.1/etc/certs/oxauth-keys.jks\n[2017-05-04 12:24:40,563: INFO/Worker-1] [chan 0] sftp session closed.\n[2017-05-04 12:24:40,564: WARNING/Worker-1] JKS file has been copied to 128.199.116.221\n[2017-05-04 12:26:31,410: WARNING/Worker-1] key rotation task will be executed approximately at 2017-05-05 12:24:40.052739 UTC  Here's an example of successful key rotation logged in  /var/log/celery/w1-1.log  when using  oxeleven  backend type:  [2017-05-04 12:47:32,727: WARNING/Worker-4] key rotation task will be executed approximately at 2017-05-05 12:24:40.052739 UTC\n[2017-05-04 12:49:07,780: WARNING/Worker-4] deleting old keys\n[2017-05-04 12:49:07,798: INFO/Worker-4] Starting new HTTP connection (1): 128.199.225.90\n[2017-05-04 12:49:08,068: WARNING/Worker-4] obtaining new keys\n[2017-05-04 12:49:08,070: INFO/Worker-4] Starting new HTTP connection (1): 128.199.225.90\n[2017-05-04 12:49:08,707: WARNING/Worker-4] pub keys has been updated  Note, if we don't see logs about key rotation in  /var/log/celery/w1-1.log , we can try to tail the other log files;  /var/log/celery/w1-2.log ,  /var/log/celery/w1-3.log , or  /var/log/celery/w1-4.log .",
            "title": "Monitoring oxAuth Key Rotation"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/",
            "text": "Configuring replicatoin between Gluu CE server using Cluster Manager\n#\n\n\nPre-requisites\n#\n\n\n\n\nGluu Server or LDIF Export\n - This Gluu server or the LDIF export of the Gluu Server's contents would be used as the main source of LDAP data.\n\n\nProvider Server\n - Vanilla Linux server with Symas OpenLDAP installed and \nsymas-openldap.conf\n configured. (1 for Delta-Syncrepl configuration and 2 for MirrorMode configuration)\n\n\nConsumer Servers\n - Vanilla Linux servers with Symas OpenLDAP installed and \nsymas-openldap.conf\n configured. (As many servers as it is required)\n\n\n\n\nDelta-Syncrepl Mode\n#\n\n\nFirst Steps\n#\n\n\n\n\nOpen the Gluu Cluster Manager in the browser. The user is welcomed with a page which gives the options to create two types of clusters.\n\n\nClick \nSetup\n from the Delta-Syncrepl box.\n\n\nIf this is the first run of the application, you will be redirected to the configuration page to setup the user dn and the password for the replication user which will be used across the cluster for replication. Configure it.\n\n\n\n\nSetting up the provider\n#\n\n\n\n\nThen you are redirected to \"New Provider\" page. Enter the details and submit the form.\n\n\nThis is a Gluu Server\n - Check this box if the server is a Gluu Server and the OpenLDAP is inside chroot container.\n\n\n(Optional) \nGluu Server Version\n - If the above checkbox is marked, then the version of Gluu-Server will have to be selected here.\n\n\nHostname\n - The hostname of the LDAP server\n\n\nIP Address\n - The IP address of the LDAP server\n\n\nPort\n - The port in which OpenLDAP is listening. Typical Gluu server would have \n1636\n\n\nLDAP Admin Password\n - The password for LDAP server. For a Gluu server, this would be the password of Admin you created during installation via \nsetup.py\n\n\nLDAP Connection Protocol\n - OpenLDAP supports plain \nldap\n and \nldaps\n protocols. Gluu Server uses \nldaps\n\n\nTLS CA Certificate\n - If you are using SSL/TLS via \nldaps\n, you will need to configure the TLS certificate locations. Enter location of CA cert on your LDAP server. For a typical Gluu server this would be \n/etc/certs/openldap.pem\n or \n/etc/certs/openldap.crt\n as it is self-signed.\n\n\nTLS Server Certificate\n - The location of server certificate. For typical gluu server it is \n/etc/certs/openldap.crt\n\n\nTLS Server Key\n - The location of server certificate's key. For typical gluu server it is \n/etc/certs/openldap.key\n\n\n\n\n\n\n\n\nThe app creates a slapd.conf using your data. You can add/change any configuration specific to your requirements and click \nSetup Server\n\n\nThe server is setup. Click \nGo to Dashboard\n once the setup is complete. In case the setup throws some error, make necessary changes in the server via ssh, then refresh the page or click the retry button at the bottom of the page to run the setup again.\n\n\nThe dashboard lists the server added. Click \nInitialize\n -> \nUsing Exported LDIF\n.\n\n\nUpload the \nalldata.ldif\n we exported from the Gluu Server. The cluster manager will import the LDIF data into the provider, and add the replication manager.\n\n\nOnce initialisation is complete an \nAdd Consumer\n shows in the dashboard in the Actions box of the provider.\n\n\n\n\nSetting up the Consumers\n#\n\n\n\n\nClick \nAdd Consumer\n next to the provider in the dashboard.\n\n\nFill in the details and submit the form for the consumer \nslapd.conf\n to be generated. As with the provider, make any special changes required and click Setup Server.\n\n\nThe consumer should be setup and configured. In case of errors, you can ssh into the server fix them and refresh the page to rerun the setup. If successfully setup, you would be asked to go to the Dashboard.\n\n\n\n\nTesting the replication\n#\n\n\n\n\nIn the Dashboard, click \nStart Test\n under \nTest Replication\n. This adds a test entry to the provider, verifies the data is replicated to all its consumers, removes the data from the provider and ensures, it is removed from all the consumers as well.\n\n\n\n\nMirror Mode\n#\n\n\n\n\nOpen Cluster Manager in the browser. Click the \nSetup\n button in the MirroMode box.\n    \n\n\nIf cluster manager hasn't been configured earlier, the user will be prompted to configure it before setting up the cluster. The values \nReplication Manager DN\n and the \nPassword\n are mandatory. Other fields are optional. Enter the values and \nUpdate Configuration\n.\n    \n\n\nNow a \nNew Provider\n can be added to the cluster. The fields are:\n\n\nThis is a Gluu Server\n - Check this box if the server is a Gluu Server and the OpenLDAP is inside chroot container.\n\n\n(Optional) \nGluu Server Version\n - If the above checkbox is marked, then the version of Gluu-Server will have to be selected here.\n\n\nHostname\n - The hostname of the LDAP server\n\n\nIP Address\n - The IP address of the LDAP server\n\n\nPort\n - The port in which OpenLDAP is listening. Typical Gluu server would have \n1636\n\n\nLDAP Admin Password\n - The password for LDAP server. For a Gluu server, this would be the password of Admin you created during installation via \nsetup.py\n\n\nLDAP Connection Protocol\n - OpenLDAP supports plain \nldap\n and \nldaps\n protocols. Gluu Server uses \nldaps\n\n\nTLS CA Certificate\n - If you are using SSL/TLS via \nldaps\n, you will need to configure the TLS certificate locations. Enter location of CA cert on your LDAP server. For a typical Gluu server this would be \n/etc/certs/openldap.pem\n or \n/etc/certs/openldap.crt\n as it is self-signed.\n\n\nTLS Server Certificate\n - The location of server certificate. For typical gluu server it is \n/etc/certs/openldap.crt\n\n\nTLS Server Key\n - The location of server certificate's key. For typical gluu server it is \n/etc/certs/openldap.key\n\n\n\n\n\n\n\n\nClick \nGenerate Provider Config\n. This generates the \nslapd.conf\n file that would be used to setup the OpenLDAP. If the organization/admin wishes to make any specific changes, they can do so here.\n    \n\n\nIf \nslapd.conf\n is satisfactory, then clicking \nSetup Server\n would setup the server ready for cluster usage. Wait until the \nAdd Mirror\n appears at the bottom of the page.\n    \n\n    \n\n\nClick the \nAdd Mirror\n button and repeat the above step 3 to step 5 to setup the mirror server.\n\n\nOnce both the servers are setup for replication, we can initialize replication by adding the replication user. Go to dashboard and Initialize \nboth the servers\n\n\nClick \nInitialize\n and select \nUsing existing data\n if the provider LDAP already has the data (in case of gluu server), or \nUsing LDIF Data\n if have some LDIF data backup that should be uploaded and used as the database.\n    \n\n    \n\n\n\n\nTroubleshooting\n#\n\n\n\n\nFor most part the web interface would log the errors encountered during the setup processes.\n\n\nIn case of some consumer misbehaving or is misconfigured, remove the server from the \ndashboard using the \nRemove\n button and re-add the server using \nAdd Consumer\n\n\n\n\nBefore adding the consumer remove the data and provider certificate from the consumer server(Server 2).\n\n\n#service solserver stop\n\n\n#rm -rf /opt/gluu/data\n\n\n#rm /opt/symas/ssl/<provider.crt>\n\n\n\n\n\n\n\n\nNote\n\n\nNot required to start LDAP manually, once the data and certificate is removed, this task would be taken care by \ncluster mananger once the consumer is added the cluster manager web application.",
            "title": "Replication"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#configuring-replicatoin-between-gluu-ce-server-using-cluster-manager",
            "text": "",
            "title": "Configuring replicatoin between Gluu CE server using Cluster Manager"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#pre-requisites",
            "text": "Gluu Server or LDIF Export  - This Gluu server or the LDIF export of the Gluu Server's contents would be used as the main source of LDAP data.  Provider Server  - Vanilla Linux server with Symas OpenLDAP installed and  symas-openldap.conf  configured. (1 for Delta-Syncrepl configuration and 2 for MirrorMode configuration)  Consumer Servers  - Vanilla Linux servers with Symas OpenLDAP installed and  symas-openldap.conf  configured. (As many servers as it is required)",
            "title": "Pre-requisites"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#delta-syncrepl-mode",
            "text": "",
            "title": "Delta-Syncrepl Mode"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#first-steps",
            "text": "Open the Gluu Cluster Manager in the browser. The user is welcomed with a page which gives the options to create two types of clusters.  Click  Setup  from the Delta-Syncrepl box.  If this is the first run of the application, you will be redirected to the configuration page to setup the user dn and the password for the replication user which will be used across the cluster for replication. Configure it.",
            "title": "First Steps"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#setting-up-the-provider",
            "text": "Then you are redirected to \"New Provider\" page. Enter the details and submit the form.  This is a Gluu Server  - Check this box if the server is a Gluu Server and the OpenLDAP is inside chroot container.  (Optional)  Gluu Server Version  - If the above checkbox is marked, then the version of Gluu-Server will have to be selected here.  Hostname  - The hostname of the LDAP server  IP Address  - The IP address of the LDAP server  Port  - The port in which OpenLDAP is listening. Typical Gluu server would have  1636  LDAP Admin Password  - The password for LDAP server. For a Gluu server, this would be the password of Admin you created during installation via  setup.py  LDAP Connection Protocol  - OpenLDAP supports plain  ldap  and  ldaps  protocols. Gluu Server uses  ldaps  TLS CA Certificate  - If you are using SSL/TLS via  ldaps , you will need to configure the TLS certificate locations. Enter location of CA cert on your LDAP server. For a typical Gluu server this would be  /etc/certs/openldap.pem  or  /etc/certs/openldap.crt  as it is self-signed.  TLS Server Certificate  - The location of server certificate. For typical gluu server it is  /etc/certs/openldap.crt  TLS Server Key  - The location of server certificate's key. For typical gluu server it is  /etc/certs/openldap.key     The app creates a slapd.conf using your data. You can add/change any configuration specific to your requirements and click  Setup Server  The server is setup. Click  Go to Dashboard  once the setup is complete. In case the setup throws some error, make necessary changes in the server via ssh, then refresh the page or click the retry button at the bottom of the page to run the setup again.  The dashboard lists the server added. Click  Initialize  ->  Using Exported LDIF .  Upload the  alldata.ldif  we exported from the Gluu Server. The cluster manager will import the LDIF data into the provider, and add the replication manager.  Once initialisation is complete an  Add Consumer  shows in the dashboard in the Actions box of the provider.",
            "title": "Setting up the provider"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#setting-up-the-consumers",
            "text": "Click  Add Consumer  next to the provider in the dashboard.  Fill in the details and submit the form for the consumer  slapd.conf  to be generated. As with the provider, make any special changes required and click Setup Server.  The consumer should be setup and configured. In case of errors, you can ssh into the server fix them and refresh the page to rerun the setup. If successfully setup, you would be asked to go to the Dashboard.",
            "title": "Setting up the Consumers"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#testing-the-replication",
            "text": "In the Dashboard, click  Start Test  under  Test Replication . This adds a test entry to the provider, verifies the data is replicated to all its consumers, removes the data from the provider and ensures, it is removed from all the consumers as well.",
            "title": "Testing the replication"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#mirror-mode",
            "text": "Open Cluster Manager in the browser. Click the  Setup  button in the MirroMode box.\n      If cluster manager hasn't been configured earlier, the user will be prompted to configure it before setting up the cluster. The values  Replication Manager DN  and the  Password  are mandatory. Other fields are optional. Enter the values and  Update Configuration .\n      Now a  New Provider  can be added to the cluster. The fields are:  This is a Gluu Server  - Check this box if the server is a Gluu Server and the OpenLDAP is inside chroot container.  (Optional)  Gluu Server Version  - If the above checkbox is marked, then the version of Gluu-Server will have to be selected here.  Hostname  - The hostname of the LDAP server  IP Address  - The IP address of the LDAP server  Port  - The port in which OpenLDAP is listening. Typical Gluu server would have  1636  LDAP Admin Password  - The password for LDAP server. For a Gluu server, this would be the password of Admin you created during installation via  setup.py  LDAP Connection Protocol  - OpenLDAP supports plain  ldap  and  ldaps  protocols. Gluu Server uses  ldaps  TLS CA Certificate  - If you are using SSL/TLS via  ldaps , you will need to configure the TLS certificate locations. Enter location of CA cert on your LDAP server. For a typical Gluu server this would be  /etc/certs/openldap.pem  or  /etc/certs/openldap.crt  as it is self-signed.  TLS Server Certificate  - The location of server certificate. For typical gluu server it is  /etc/certs/openldap.crt  TLS Server Key  - The location of server certificate's key. For typical gluu server it is  /etc/certs/openldap.key     Click  Generate Provider Config . This generates the  slapd.conf  file that would be used to setup the OpenLDAP. If the organization/admin wishes to make any specific changes, they can do so here.\n      If  slapd.conf  is satisfactory, then clicking  Setup Server  would setup the server ready for cluster usage. Wait until the  Add Mirror  appears at the bottom of the page.\n     \n      Click the  Add Mirror  button and repeat the above step 3 to step 5 to setup the mirror server.  Once both the servers are setup for replication, we can initialize replication by adding the replication user. Go to dashboard and Initialize  both the servers  Click  Initialize  and select  Using existing data  if the provider LDAP already has the data (in case of gluu server), or  Using LDIF Data  if have some LDIF data backup that should be uploaded and used as the database.",
            "title": "Mirror Mode"
        },
        {
            "location": "/replication/Setting-up-LDAP-replication/#troubleshooting",
            "text": "For most part the web interface would log the errors encountered during the setup processes.  In case of some consumer misbehaving or is misconfigured, remove the server from the \ndashboard using the  Remove  button and re-add the server using  Add Consumer   Before adding the consumer remove the data and provider certificate from the consumer server(Server 2).  #service solserver stop  #rm -rf /opt/gluu/data  #rm /opt/symas/ssl/<provider.crt>     Note  Not required to start LDAP manually, once the data and certificate is removed, this task would be taken care by \ncluster mananger once the consumer is added the cluster manager web application.",
            "title": "Troubleshooting"
        },
        {
            "location": "/troubleshooting/troubleshooting/",
            "text": "Failure on LDAP Replication\n#\n\n\nOn first try, replication between provider and consumer might be failed (an error message \n\"Test data is NOT replicated.\"\n appeared in Cluster Manager web app). \n\n\nTo investigate the issue:\n\n\n\n\nLogin to consumer server.\n\n\n\n\nssh root@<server>\n\n\n\n\n\n\nCheck OpenLDAP log.\n\n\n\n\ntail -n 100 /var/log/openldap/ldap.log\n\n\n\n\n\n\nIf we can't find / unsure the cause of the error, restart the OpenLDAP.\n\n\n\n\nservice solserver restart\n\n\n\n\n\n\n\n\nRetry the replication via Cluster Manager web app.\n\n\n\n\n\n\nIf the problem persists, stop OpenLDAP.\n\n\n\n\n\n\nservice solserver stop\n\n\n\n\n\n\nStart OpenLDAP with debug mode (OpenLDAP will display logs)\n\n\n\n\nservice solserver start -d 1\n\n\n\n\n\n\n\n\nRetry the replication via Cluster Manager web app while investigating the logs mentioned in step 6.\n\n\n\n\n\n\nDo necessary action to fix the problem pointed in the logs.\n\n\n\n\n\n\nIf the problem is gone, make sure to stop OpenLDAP (using CTRL+C) then run \nservice solserver restart\n.",
            "title": "Troubleshooting Issues"
        },
        {
            "location": "/troubleshooting/troubleshooting/#failure-on-ldap-replication",
            "text": "On first try, replication between provider and consumer might be failed (an error message  \"Test data is NOT replicated.\"  appeared in Cluster Manager web app).   To investigate the issue:   Login to consumer server.   ssh root@<server>   Check OpenLDAP log.   tail -n 100 /var/log/openldap/ldap.log   If we can't find / unsure the cause of the error, restart the OpenLDAP.   service solserver restart    Retry the replication via Cluster Manager web app.    If the problem persists, stop OpenLDAP.    service solserver stop   Start OpenLDAP with debug mode (OpenLDAP will display logs)   service solserver start -d 1    Retry the replication via Cluster Manager web app while investigating the logs mentioned in step 6.    Do necessary action to fix the problem pointed in the logs.    If the problem is gone, make sure to stop OpenLDAP (using CTRL+C) then run  service solserver restart .",
            "title": "Failure on LDAP Replication"
        }
    ]
}